{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-AZ-Sec34-NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_LulNCC8z96"
      },
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "1.   Apply the Bag-of-words model to analyze the restaurant reviews with different classification models\n",
        "\n",
        "2.   **Input** = Restaurant_Reviews.tsv\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpXdowrE9DxW"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhKd4hWx9GFt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt       #graphs\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzLuJf0B6Tg1"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCX22pc2F8Cz"
      },
      "source": [
        "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GelWbapE6-cM",
        "outputId": "d3d813a7-73c5-45e1-f01b-08a78d78082f"
      },
      "source": [
        "print(dataset)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Review  Liked\n",
            "0                             Wow... Loved this place.      1\n",
            "1                                   Crust is not good.      0\n",
            "2            Not tasty and the texture was just nasty.      0\n",
            "3    Stopped by during the late May bank holiday of...      1\n",
            "4    The selection on the menu was great and so wer...      1\n",
            "..                                                 ...    ...\n",
            "995  I think food should have flavor and texture an...      0\n",
            "996                           Appetite instantly gone.      0\n",
            "997  Overall I was not impressed and would not go b...      0\n",
            "998  The whole experience was underwhelming, and I ...      0\n",
            "999  Then, as if I hadn't wasted enough of my life ...      0\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E--Vc-xXF-Si"
      },
      "source": [
        "# Cleaning the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJU5M7dX7FKU",
        "outputId": "ac75583e-049d-4bfc-9d33-31077592b0a4"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords #meaningless words (articles...)\n",
        "from nltk.stem.porter import PorterStemmer #simplify the text by converting the words to their root"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ur76fX8KEb"
      },
      "source": [
        "corpus = []\n",
        "for i in range(0, len(dataset)):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
        "  review = review.lower()\n",
        "  review = review.split()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIUdTnYq9AWS",
        "outputId": "766084f2-569f-4117-a3bd-c2221bc4d4cb"
      },
      "source": [
        "print(review)\n",
        "print(dataset['Review'][999])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['then', 'as', 'if', 'i', 'hadn', 't', 'wasted', 'enough', 'of', 'my', 'life', 'there', 'they', 'poured', 'salt', 'in', 'the', 'wound', 'by', 'drawing', 'out', 'the', 'time', 'it', 'took', 'to', 'bring', 'the', 'check']\n",
            "Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing out the time it took to bring the check.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FS_CMupdWvR"
      },
      "source": [
        "#Creating the Bag of Words model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsgM4SdJi-XS"
      },
      "source": [
        "##Splitting the dataset into the Training set and Test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReckG8Wi5Jv3"
      },
      "source": [
        "## Training the Naive Bayes model on the Training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuE_MUTI5NRf"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt2L3XMc5PfW"
      },
      "source": [
        "##Making the Confusion Matrix"
      ]
    }
  ]
}